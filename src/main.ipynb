{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- Import the packages and functions\n",
    "from tools.functions import *\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitions, parameters and directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definitions, parameters and directory paths\n",
    "# Define the base path for your images\n",
    "base_path       = '../img'\n",
    "\n",
    "# Define the path to your dataset within the base path and using gray images\n",
    "dataset_path    = f'{base_path}/dataset/gray/'\n",
    "\n",
    "# List of ratios to be used for splitting the dataset into training and testing sets\n",
    "train_ratio     = [0.75, 0.8, 0.85, 0.9]\n",
    "\n",
    "# Threshold for normalizing the image data\n",
    "thresh_normalization = 0.500\n",
    "\n",
    "# List of thresholds to be considered 'good' in evaluation\n",
    "thresh_good     = [0.00, 0.10, 0.15]\n",
    "\n",
    "# Range of iterations to be performed during model training\n",
    "iterations      = range(1, 6)\n",
    "\n",
    "# Define different classification types for the model to learn. Each type is associated with a list of categories.\n",
    "classification_types = {'n_grains': ['50', '60', '70', '80', '90', '100'], \n",
    "                        'defect_stratified': ['0%', '10%', '15%', '20%', '25%', '30%'], \n",
    "                        'defect_thresholded': ['With defects', 'Healthy']}\n",
    "\n",
    "# Define labels for the x and y axes in plotting\n",
    "x_label = 'Predicted'\n",
    "y_label = 'True'\n",
    "\n",
    "## ---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all image filenames in the directory specified in dataset_path\n",
    "image_filenames = read_all_images_filenames(dataset_path)\n",
    "\n",
    "# Initialize an empty DataFrame for storing training data\n",
    "df_train                    = pd.DataFrame()\n",
    "\n",
    "# Initialize an empty DataFrame for storing testing data\n",
    "df_test                     = pd.DataFrame()\n",
    "\n",
    "# Initialize an empty DataFrame for storing model's training data\n",
    "df_train_model              = pd.DataFrame()\n",
    "\n",
    "# Initialize an empty DataFrame for storing classification results\n",
    "classification_results_df   = pd.DataFrame()\n",
    "\n",
    "# Initialize a nested dictionary for storing classification metrics results. \n",
    "# The structure is organized first by training ratio, then by threshold, and finally by classification type\n",
    "classification_metrics_results = {\n",
    "    ratio: {\n",
    "        thresh: {\n",
    "            classification_type: {} for classification_type in classification_types\n",
    "        } for thresh in thresh_good\n",
    "    } for ratio in train_ratio\n",
    "}\n",
    "\n",
    "# Initialize a nested dictionary for storing confusion matrices. \n",
    "# The structure is similar to that of classification_metrics_results\n",
    "confusion_matrices = {\n",
    "    ratio: {\n",
    "        thresh: {\n",
    "            classification_type: {} for classification_type in classification_types\n",
    "        } for thresh in thresh_good\n",
    "    } for ratio in train_ratio\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates the model and performs the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ratio_80to255_by_1to80'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\Engenharia Eletronica\\TCC2\\scripts\\maize-classification\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\Engenharia Eletronica\\TCC2\\scripts\\maize-classification\\venv\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Engenharia Eletronica\\TCC2\\scripts\\maize-classification\\venv\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ratio_80to255_by_1to80'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m df_test \u001b[39m=\u001b[39m create_dataframes(test_filenames, dataset_path)\n\u001b[0;32m     17\u001b[0m \u001b[39m# Normalize the 'ratio_80to255_by_1to80' feature in the train and test datasets. The normalization threshold is specified\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m df_train, df_test \u001b[39m=\u001b[39m normalize_dataset(df_train, df_test, \u001b[39m'\u001b[39;49m\u001b[39mratio_80to255_by_1to80\u001b[39;49m\u001b[39m'\u001b[39;49m, thresh_normalization)\n\u001b[0;32m     20\u001b[0m \u001b[39m# Summarize the train dataframe by grouping it by grain_quantity and defect_percentage and calculating summary statistics to generate the model\u001b[39;00m\n\u001b[0;32m     21\u001b[0m df_train_model \u001b[39m=\u001b[39m summarize_train_data(df_train)\n",
      "File \u001b[1;32md:\\Engenharia Eletronica\\TCC2\\scripts\\maize-classification\\src\\tools\\functions.py:84\u001b[0m, in \u001b[0;36mnormalize_dataset\u001b[1;34m(df_train, df_test, feature_name, ratio_to_be_filtered)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalize_dataset\u001b[39m(df_train, df_test, feature_name, ratio_to_be_filtered \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m):\n\u001b[1;32m---> 84\u001b[0m     good_grain_stats \u001b[39m=\u001b[39m df_train\u001b[39m.\u001b[39;49mquery(\u001b[39m'\u001b[39;49m\u001b[39mdefect_percentage == 0.0\u001b[39;49m\u001b[39m'\u001b[39;49m)[feature_name]\u001b[39m.\u001b[39magg([\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstd\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     86\u001b[0m     df_train[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnormalized_\u001b[39m\u001b[39m{\u001b[39;00mfeature_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(good_grain_stats[\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m-\u001b[39m df_train[feature_name])\u001b[39m/\u001b[39mgood_grain_stats[\u001b[39m'\u001b[39m\u001b[39mstd\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     87\u001b[0m     df_test[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnormalized_\u001b[39m\u001b[39m{\u001b[39;00mfeature_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(good_grain_stats[\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m-\u001b[39m df_test[feature_name])\u001b[39m/\u001b[39mgood_grain_stats[\u001b[39m'\u001b[39m\u001b[39mstd\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32md:\\Engenharia Eletronica\\TCC2\\scripts\\maize-classification\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32md:\\Engenharia Eletronica\\TCC2\\scripts\\maize-classification\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ratio_80to255_by_1to80'"
     ]
    }
   ],
   "source": [
    "# Perform the procedure for all specified training/testing percentages, thresholds to consider good grains, and the number of iterations for each set\n",
    "for ratio in train_ratio:           \n",
    "    for thresh in thresh_good:\n",
    "        for iteration in iterations:\n",
    "\n",
    "            df_train                    = pd.DataFrame()\n",
    "            df_test                     = pd.DataFrame()\n",
    "            df_train_model              = pd.DataFrame()\n",
    "\n",
    "            # Divide the dataset into train and test sets with the specified ratio\n",
    "            train_filenames, test_filenames = divide_dataset(image_filenames, ratio)\n",
    "\n",
    "            # Create dataframes for the train and test sets using the filenames and images in the specified directory\n",
    "            df_train = create_dataframes(train_filenames, dataset_path)\n",
    "            df_test = create_dataframes(test_filenames, dataset_path)\n",
    "\n",
    "            # Normalize the 'ratio_80to255_by_1to80' feature in the train and test datasets. The normalization threshold is specified\n",
    "            df_train, df_test = normalize_dataset(df_train, df_test, 'ratio_80to255_by_1to80', thresh_normalization)\n",
    "\n",
    "            # Summarize the train dataframe by grouping it by grain_quantity and defect_percentage and calculating summary statistics to generate the model\n",
    "            df_train_model = summarize_train_data(df_train)\n",
    "\n",
    "            # Call the function to calculate the average number of pixels per grain for the training dataset\n",
    "            avg_pixels_1a255_per_grain = calculate_avg_pixels_per_grain(df_train)\n",
    "\n",
    "            # Calculate number of grains\n",
    "            df_test['calculated_grain_quantity'] = df_test['npixels_1to255'].apply(calculate_number_of_grains, grain_avg = avg_pixels_1a255_per_grain)\n",
    "\n",
    "            # Estimate number of grains (in discrete values predefined)\n",
    "            df_test['estimated_grain_quantity'] = df_test['calculated_grain_quantity'].apply(estimate_number_of_grains)\n",
    "\n",
    "            # Calculate the error between the actual and estimated values for the quantity of grains\n",
    "            df_test['error_grain'] = df_test['estimated_grain_quantity'] - df_test['grain_quantity']\n",
    "\n",
    "            # Estimate the percentage of defects in the test dataset based on the number of grains and ratio\n",
    "            df_test['estimated_defect_percentage'] = df_test.apply(lambda row: estimate_defect_percentage(row['grain_quantity'], \n",
    "                                                                                                           row['normalized_ratio_80to255_by_1to80'], df_train_model), axis=1)\n",
    "\n",
    "            # Calculate the error in the estimated defect percentage\n",
    "            df_test['error_defects'] = df_test['estimated_defect_percentage'] - df_test['defect_percentage']\n",
    "\n",
    "            # Select only the columns of interest to create a summarized dataframe with the classification results\n",
    "            classification_results_df = df_test[['grain_quantity', 'defect_percentage', 'estimated_grain_quantity', 'estimated_defect_percentage', 'error_grain']].copy()\n",
    "\n",
    "            # Check the quality (healthy or defective) according to the parameterized threshold\n",
    "            classification_results_df['quality'] = (classification_results_df['defect_percentage'].apply(check_quality, thresh)).astype(int)\n",
    "            classification_results_df['estimated_quality'] = (classification_results_df['estimated_defect_percentage'].apply(check_quality, thresh)).astype(int)\n",
    "            \n",
    "            # Iterate over each classification type and get the classification results\n",
    "            for classification_type in classification_types:\n",
    "\n",
    "                cm, cr = generate_confusion_matrix_and_classification_metrics(classification_results_df, classification_types[classification_type], classification_type)\n",
    "                \n",
    "                # Update the confusion matrix and classification metrics dictionaries\n",
    "                confusion_matrices[ratio][thresh][classification_type].update({iteration: cm})\n",
    "                classification_metrics_results[ratio][thresh][classification_type].update({iteration: cr})\n",
    "\n",
    "# Apply a cross-validation by taking the average of the results obtained in each iteration\n",
    "classification_metrics_results, confusion_matrices = cross_validation(train_ratio, thresh_good, classification_types, \n",
    "                                                                      iterations, classification_metrics_results, confusion_matrices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_all_confusion_matrices_images(train_ratio, thresh_good, classification_types, iterations, confusion_matrices, base_path, [x_label, y_label])\n",
    "\n",
    "with open(\"../resources/classification_metrics_results.pickle\", \"wb\") as f:\n",
    "    pickle.dump(classification_metrics_results, f)\n",
    "\n",
    "with open(\"../resources/confusion_matrices.pickle\", \"wb\") as f:\n",
    "    pickle.dump(confusion_matrices, f)\n",
    "\n",
    "df_test.to_pickle('../resources/df_test.pkl')\n",
    "df_train.to_pickle('../resources/df_train.pkl')\n",
    "df_train_model.to_pickle('../resources/df_train_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8067347c3ab30bb8892547d646c700eb911e2c3a99595b0254859100a92c716a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
